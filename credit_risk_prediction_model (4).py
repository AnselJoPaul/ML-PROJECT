# -*- coding: utf-8 -*-
"""Credit Risk Prediction Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WJy-P27YwfZ7LSFhCXzDHqmG1PHoHeys
"""

import pandas as pd
import requests

# URL of the shared Google Drive link
url = 'https://drive.google.com/uc?export=download&id=1D1u7i2UYaCVQSizYtasp39EFyVwDJbel'

r = requests.get(url)
with open('german_credit_data_labeled.csv', 'wb') as f:
    f.write(r.content)

# Load the downloaded file
df = pd.read_csv('german_credit_data_labeled.csv')

# Show the first few rows
df.head()

"""Explore"""

# Check basic info about the dataset
df.info()

# See column-wise missing value counts
print("\n Missing values in each column:")
print(df.isnull().sum())

# Summary stats for numerical features
df.describe()

# Check unique values in the target column
print("\n Unique values in 'Risk' column (Target):")
print(df['Credit Risk'].value_counts())

"""Preprocessing"""

from sklearn.preprocessing import LabelEncoder, StandardScaler

categorical_cols = df.select_dtypes(include=['object']).columns
categorical_cols = categorical_cols[categorical_cols != 'Risk']  # Exclude target column

label_encoder = LabelEncoder()
for col in categorical_cols:
    df[col] = label_encoder.fit_transform(df[col])

# Scaling numerical features
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns
numerical_cols = numerical_cols[numerical_cols != 'Risk']  # Exclude target column

scaler = StandardScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

# Check the updated dataset
print(df.head())

"""Split data"""

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

label_encoder = LabelEncoder()
df['Credit Risk'] = label_encoder.fit_transform(df['Credit Risk'])  # 'bad' -> 0, 'good' -> 1

print("Encoded classes:", list(label_encoder.classes_))  # ['bad', 'good']
print("\nUpdated value counts after encoding:")
print(df['Credit Risk'].value_counts())  # Should show 0 and 1 counts

# Features and target
X = df.drop('Credit Risk', axis=1)
y = df['Credit Risk']

# Stratified split to preserve class distribution
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Confirm class balance in both splits
print("\n Training data - Credit Risk distribution:")
print(y_train.value_counts())
print("\n Testing data - Credit Risk distribution:")
print(y_test.value_counts())

from sklearn.preprocessing import LabelEncoder, StandardScaler

df_clean = df.copy()

label_encoder = LabelEncoder()
for col in df_clean.select_dtypes(include='object').columns:
    df_clean[col] = label_encoder.fit_transform(df_clean[col])

print(df_clean.dtypes)

scaler = StandardScaler()
features_to_scale = df_clean.drop('Credit Risk', axis=1).columns
df_clean[features_to_scale] = scaler.fit_transform(df_clean[features_to_scale])

# Redefine X and y
X = df_clean.drop('Credit Risk', axis=1)
y = df_clean['Credit Risk']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(" Training data:\n", y_train.value_counts())
print(" Testing data:\n", y_test.value_counts())

"""Training"""

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

""" 1. Logistic Regression"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print(" Logistic Regression Results:")
print(classification_report(y_test, y_pred_lr))

# Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, fmt='d', cmap='Blues')
plt.title("Logistic Regression - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""2. Random Forest"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print(" Random Forest Results:")
print(classification_report(y_test, y_pred_rf))

sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Greens')
plt.title("Random Forest - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""3. XGBoost"""

from xgboost import XGBClassifier

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)

print(" XGBoost Results:")
print(classification_report(y_test, y_pred_xgb))

sns.heatmap(confusion_matrix(y_test, y_pred_xgb), annot=True, fmt='d', cmap='Oranges')
plt.title("XGBoost - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""4. Support Vector Machine (SVM)"""

from sklearn.svm import SVC

svm = SVC(kernel='rbf', probability=True)
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)

print("SVM Results:")
print(classification_report(y_test, y_pred_svm))

sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, fmt='d', cmap='Purples')
plt.title("SVM - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

""" 5. Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(random_state=42)
gb.fit(X_train, y_train)
y_pred_gb = gb.predict(X_test)

print(" Gradient Boosting Results:")
print(classification_report(y_test, y_pred_gb))

sns.heatmap(confusion_matrix(y_test, y_pred_gb), annot=True, fmt='d', cmap='Reds')
plt.title("Gradient Boosting - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Store results in a dictionary
results = {
    "Model": [],
    "Accuracy": [],
    "Precision": [],
    "Recall": [],
    "F1 Score": []
}

models = {
    "Logistic Regression": y_pred_lr,
    "Random Forest": y_pred_rf,
    "XGBoost": y_pred_xgb,
    "SVM": y_pred_svm,
    "Gradient Boosting": y_pred_gb
}

# Calculate and store metrics
for model_name, y_pred in models.items():
    results["Model"].append(model_name)
    results["Accuracy"].append(accuracy_score(y_test, y_pred))
    results["Precision"].append(precision_score(y_test, y_pred))
    results["Recall"].append(recall_score(y_test, y_pred))
    results["F1 Score"].append(f1_score(y_test, y_pred))

# Create DataFrame for results
results_df = pd.DataFrame(results)
print("Model Performance Comparison:")
print(results_df.sort_values(by="F1 Score", ascending=False))

# Plot bar charts for comparison
plt.figure(figsize=(12, 8))
metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']

for i, metric in enumerate(metrics):
    plt.subplot(2, 2, i + 1)
    sns.barplot(x='Model', y=metric, data=results_df.sort_values(by=metric, ascending=False), palette="Set2")
    plt.xticks(rotation=30)
    plt.title(f"{metric} Comparison")
    plt.tight_layout()

plt.show()

#                Model  Accuracy  Precision    Recall  F1 Score
#              XGBoost     1.000   1.000000  1.000000  1.000000
#    Gradient Boosting     1.000   1.000000  1.000000  1.000000
#        Random Forest     0.990   0.972222  1.000000  0.985915
#                  SVM     0.915   0.907692  0.842857  0.874074
#  Logistic Regression     0.605   0.333333  0.128571  0.185567

"""Overfitting Check"""

from sklearn.model_selection import cross_val_score, StratifiedKFold

# Use 5-fold stratified cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

models_cv = {
    "Logistic Regression": LogisticRegression(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
    "SVM": SVC(kernel='rbf', probability=True),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42)
}

# Store cross-validation results
cv_results = {
    "Model": [],
    "Mean Accuracy": [],
    "Std Dev": []
}

for model_name, model in models_cv.items():
    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
    cv_results["Model"].append(model_name)
    cv_results["Mean Accuracy"].append(scores.mean())
    cv_results["Std Dev"].append(scores.std())

# Convert to DataFrame for better display
cv_df = pd.DataFrame(cv_results).sort_values(by="Mean Accuracy", ascending=False)
print("Cross-Validation Results (Accuracy):")
print(cv_df)

#ROC Curve
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize

# Get probability scores
y_probs = {
    "Logistic Regression": LogisticRegression().fit(X_train, y_train).predict_proba(X_test)[:, 1],
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42).fit(X_train, y_train).predict_proba(X_test)[:, 1],
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42).fit(X_train, y_train).predict_proba(X_test)[:, 1],
    "SVM": SVC(kernel='rbf', probability=True).fit(X_train, y_train).predict_proba(X_test)[:, 1],
    "Gradient Boosting": GradientBoostingClassifier(random_state=42).fit(X_train, y_train).predict_proba(X_test)[:, 1]
}

plt.figure(figsize=(10, 8))

for model_name, probas in y_probs.items():
    fpr, tpr, _ = roc_curve(y_test, probas)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{model_name} (AUC = {roc_auc:.2f})")

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend(loc="lower right")
plt.grid()
plt.show()

#Picked XGBoost as my final model:
#Top cross-validation accuracy - 0.991
#Perfect ROC AUC - 1.000
#Low variance - 0.0037
#Fast prediction, and works well out-of-the-box

#Runner-ups:
#Random Forest 
#SVM

import joblib

# Save the trained XGBoost model
joblib.dump(xgb, 'xgboost_credit_risk_model.pkl')

"""Top 10 Important Features (XGBoost)"""

from xgboost import plot_importance
import matplotlib.pyplot as plt

# Plot top 10 most important features
plot_importance(xgb, max_num_features=10, importance_type='gain', height=0.5)
plt.title("Top 10 Important Features (XGBoost)")
plt.show()